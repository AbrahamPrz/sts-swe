10.5 ■ operación del sistema 283 2. el enfoque de sistemas . el supuesto básico es que las personas son falibles y se equivocarán. los errores que la gente comete por lo general son consecuencia de deci- siones de diseño del sistema, que llevan a formas erróneas de trabajar, o bien, de facto-res de la organización, que afectan a los operadores del sistema. los sistemas eficaces tienen que reconocer la posibilidad del error humano, e incluir barreras y protecciones que los detecten, y permitir al sistema recuperarse antes de que ocurra la falla. cuando ésta sucede, la prioridad no es encontrar al individuo para echarle la culpa, sino enten-der cómo y por qué las protecciones del sistema pasaron por alto el error. se considera que el enfoque de sistemas es el correcto, y que los ingenieros de siste-mas deben admitir que ocurrirán errores humanos durante la operación del sistema. por ello, para mejorar la seguridad y la confiabilidad de un sistema, los diseñadores deben pensar en las protecciones y barreras al error humano que tienen que incluirse en un sistema. también es necesario considerar si dichas barreras tienen que edificarse en los procedimientos técnicos del sistema. si no es así, serían parte de los procesos y procedi-mientos para usar el sistema, o serían los lineamientos del operador los que dependen de la verificación y reflexión humanas. los siguientes son ejemplos de protecciones que pueden incluirse en un sistema: 1. un sistema de control de tráfico aéreo podría incluir un sistema automatizado de alerta de conflicto. cuando un controlador instruya a una aeronave a cambiar su velocidad o su altitud, el sistema extrapola su trayectoria para saber si podría coli-sionar contra alguna otra aeronave. si es así, se activa una alarma. 2. el mismo sistema podría tener un procedimiento claramente definido para registrar las instrucciones de control emitidas. dichas acciones ayudan al controlador a com-probar si emitieron correctamente la instrucción y a disponer de información para que la verifiquen otros. 3. el control de tráfico aéreo comúnmente implica un equipo de controladores que, de manera continua, monitorizan el trabajo de los demás. por consiguiente, cuando se comete un error, es probable que se detecte y corrija antes de que ocurra un incidente. inevitablemente, todas las barreras presentan debilidades de algún tipo. reason las llama “condiciones latentes”, pues usualmente sólo contribuyen a la falla del sistema cuando ocurre algún otro problema. por ejemplo, sobre las defensas, un punto débil de un sistema de alerta de conflicto es que puede conducir a muchas falsas alarmas. por lo tanto, los controladores ignorarían las advertencias del sistema. una inconsistencia de un sistema procedimental sería que la información inusual, pero esencial, no se registre con facilidad. la revisión humana suele fallar cuando todas las personas que intervienen enfrentan mucho estrés y cometen el mismo error. las condiciones latentes conducen a falla del sistema cuando las defensas construidas en éste no detectan una falla activa por parte de un operador del sistema. el error humano es un activador de la falla, pero no debe considerarse como la razón exclusiva de ella. reason explica lo anterior utilizando su muy conocido modelo de “queso suizo” de la falla del sistema (figura 10.9). en este modelo, las defensas construidas en un sistema se comparan con rebanadas de queso suizo. algunos tipos de queso suizo, como el emmental, tienen orificios y, por m10_sommerville_ingenieria_1ed_se_261-288.indd  283m10_sommerville_ingenieria_1ed_se_261-288.indd  283 3/18/11  4:53:19 pm3/18/11  4:53:19 pm