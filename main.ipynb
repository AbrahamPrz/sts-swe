{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se busca trabajar con 3 libros de texto en formato PDF, los cuales son:  \n",
    "Codigo limpio - Robert Cecil Martin  \n",
    "Ingenieria de software - Pressman  \n",
    "Ingenieria de software - Sommerville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in ./.venv/lib/python3.10/site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from tqdm.auto import tqdm\n",
    "from os import makedirs, listdir, path, getcwd\n",
    "\n",
    "BASE_DIR = getcwd()\n",
    "DATA_DIR = path.join(BASE_DIR, \"data\")\n",
    "PDF_DIR = path.join(DATA_DIR, \"pdf\")\n",
    "TXT_DIR = path.join(DATA_DIR, \"txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez teniendo los libros en formato PDF, se procede a convertirlos a formato TXT, para poder trabajar con ellos.\n",
    "La conversion ocurre pagina por pagina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el directorio si no existe\n",
    "makedirs(TXT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing codigo_limpio_robert_cecil_martin.pdf: 100%|██████████| 644/644 [00:13<00:00, 47.41it/s] \n",
      "Processing ingenieria_de_software_sommerville.PDF: 100%|██████████| 792/792 [00:03<00:00, 200.58it/s]\n",
      "Processing ingenieria_de_software_pressman.PDF: 100%|██████████| 810/810 [00:08<00:00, 92.95it/s] \n"
     ]
    }
   ],
   "source": [
    "pdf_files = [f for f in listdir(PDF_DIR) if f.lower().endswith('.pdf')]\n",
    "for pdf in pdf_files:\n",
    "    pdf_reader = PdfReader((path.join(PDF_DIR, pdf)))\n",
    "    progress_bar = tqdm(range(len(pdf_reader.pages)))\n",
    "    progress_bar.set_description(f'Processing {pdf}')\n",
    "    text = ' '.join([pdf_reader.pages[page].extract_text().replace('\\n', ' ') for page in progress_bar])\n",
    "    with open(path.join(TXT_DIR, f'{pdf[:-4]}.txt'), 'w') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez teniendo todos los archivos .txt se debe obtener el dataset de STS lleno de oraciones relacionadas a la ingenieria de software y un puntaje entre 0 y 1 que valore la similitud entre las oraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se buscara hacer uso de algun modelo para extraer las oraciones del archivo .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.10/site-packages (0.15.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.30.2)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (2.13.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (3.12.2)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.6.3)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.10/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub transformers datasets  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscando los modelos en espanol para extraer automaticamente las oraciones de ingenieria de software encontre este modelo: [hiiamsid/sentence_similarity_spanish_es](https://huggingface.co/hiiamsid/sentence_similarity_spanish_es) el cual potencialmente sera usado para realizarle fine-tuning y asi obtener un modelo final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se busca obtener cada oracion del archivo .txt principal de cada libro e incluirla en un archivo .json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se limpia el texto de cada archivo .txt, se eliminan los saltos de linea, espacios en blanco, caracteres especiales, palabras que no aportan informacion, palabras repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_useless_text(text):\n",
    "    # Regular expressions for detecting useless text patterns\n",
    "    patterns = [\n",
    "        r'>',  # Remove bullet point indicators\n",
    "        r'\\d+\\w*',  # Remove numbers or digits combined with letters\n",
    "        r'\\b\\w\\b',  # Remove single-letter words\n",
    "        r'\\b\\w{1,2}\\b',  # Remove words with one or two characters\n",
    "        r'www\\.[^\\s]+',  # Remove URLs\n",
    "    ]\n",
    "\n",
    "    # Apply each pattern sequentially to remove useless text\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEA RAPIDA.\n",
    "\n",
    "USAR UN TOKENIZER EN ESPANOL PARA TOKENIZAR POR SUBWORD Y LIMPIAR EL TEXTO CON ESO COMO ALTERNATIVA A remove_useless_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch) (4.6.3)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch)\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch)\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch)\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch)\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch)\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0 (from torch)\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.0)\n",
      "Collecting wheel (from nvidia-cublas-cu11==11.10.3.66->torch)\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting cmake (from triton==2.0.0->torch)\n",
      "  Using cached cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "Collecting lit (from triton==2.0.0->torch)\n",
      "  Using cached lit-16.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, lit, cmake, wheel, sympy, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, networkx, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.26.4 lit-16.0.6 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0 wheel-0.40.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1908781 1908781\n",
      "670110 670110\n"
     ]
    }
   ],
   "source": [
    "for filename in listdir(TXT_DIR):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        cleaned_text = remove_useless_text(text)\n",
    "        print(len(cleaned_text), len(text))\n",
    "        # Write the cleaned text to a new file\n",
    "        cleaned_file_path = path.join(TXT_DIR, f'cleaned_{filename}')\n",
    "        with open(cleaned_file_path, 'w', encoding='utf-8') as cleaned_file:\n",
    "            cleaned_file.write(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "not readable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m file_path \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39mjoin(TXT_DIR, filename)\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path\u001b[39m.\u001b[39mjoin(TXT_DIR, filename), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m----> 6\u001b[0m     text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m      7\u001b[0m     cleaned_text \u001b[39m=\u001b[39m remove_useless_text(text)\n\u001b[1;32m      8\u001b[0m     file\u001b[39m.\u001b[39mwrite(cleaned_text)\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: not readable"
     ]
    }
   ],
   "source": [
    "\n",
    "for filename in listdir(TXT_DIR):\n",
    "    if not any(char.isdigit() for char in filename):\n",
    "        # por la convencion de nombres que se tomo, estos archivos son los que contienen todo el texto de su respectivo pdf\n",
    "        file_path = path.join(TXT_DIR, filename)\n",
    "        with open(path.join(TXT_DIR, filename), 'rw', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            cleaned_text = remove_useless_text(text)\n",
    "            file.write\n",
    "            # Do something with the cleaned text, such as write it to a new file\n",
    "            with open(file_path, 'w', encoding='utf-8') as cleaned_file:\n",
    "                cleaned_file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con un archivo que contiene texto \"limpio\" se obtienen las oraciones de cada libro y se incluyen en un archivo .json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import json\n",
    "\n",
    "# # Descargar el tokenizador de oraciones de NLTK\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# with open('/home/abraham/personal/sts-swe/data/txt/cleaned_codigo_limpio_robert_cecil_martin.txt', 'r') as file:\n",
    "#     text = file.read()\n",
    "\n",
    "# sentences = nltk.sent_tokenize(text)\n",
    "# meaningful_sentences = []\n",
    "# for sentence in sentences:\n",
    "#     # Si la oración tiene más de 5 palabras, la consideramos significativa\n",
    "#     if len(sentence.split()) > 5:\n",
    "#         # Decodificar la oración para reemplazar los escape sequences con los caracteres correspondientes\n",
    "#         sentence = sentence.encode().decode('unicode_escape')\n",
    "#         meaningful_sentences.append(sentence)\n",
    "\n",
    "# # Crear un objeto JSON y agregar las oraciones significativas como elementos del objeto\n",
    "# data = {}\n",
    "# data['sentences'] = meaningful_sentences\n",
    "\n",
    "# with open('meaningful_sentences.json', 'w') as file:\n",
    "#     json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in ./.venv/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./.venv/lib/python3.10/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.10/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./.venv/lib/python3.10/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./.venv/lib/python3.10/site-packages (from spacy) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./.venv/lib/python3.10/site-packages (from spacy) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting es-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.5.0/es_core_news_sm-3.5.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from es-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] http:// <<\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "\n",
    "# Load Spanish tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"es_core_news_sm\")  \n",
    "# you can change \"es_core_news_sm\" to \"es_core_news_md\" if you have downloaded the medium Spanish model\n",
    "\n",
    "# Increase the max_length limit to 2000000\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "# Read the text file\n",
    "with open('/home/abraham/personal/sts-swe/data/txt/cleaned_codigo_limpio_robert_cecil_martin.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Process whole documents\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize sentence\n",
    "sentences = [sent.text for sent in doc.sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_sentences = []\n",
    "for sentence in sentences:\n",
    "    # Si la oración tiene más de 5 palabras, la consideramos significativa\n",
    "    if len(sentence.split()) > 5:\n",
    "        # Decodificar la oración para reemplazar los escape sequences con los caracteres correspondientes\n",
    "        sentence = sentence.encode().decode('unicode_escape')\n",
    "        meaningful_sentences.append(sentence)\n",
    "\n",
    "# Crear un objeto JSON y agregar las oraciones significativas como elementos del objeto\n",
    "data = {}\n",
    "data['sentences'] = meaningful_sentences\n",
    "\n",
    "with open('meaningful_sentences.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se opta por usar los embeddings del modelo anteriormente mencionado para asi encontrar el contexto de cada oracion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
