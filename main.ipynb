{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se busca trabajar con 3 libros de texto en formato PDF, los cuales son:  \n",
    "Codigo limpio - Robert Cecil Martin  \n",
    "Ingenieria de software - Pressman  \n",
    "Ingenieria de software - Sommerville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in ./.venv/lib/python3.10/site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from tqdm.auto import tqdm\n",
    "from os import makedirs, listdir, path, getcwd\n",
    "\n",
    "BASE_DIR = getcwd()\n",
    "DATA_DIR = path.join(BASE_DIR, \"data\")\n",
    "PDF_DIR = path.join(DATA_DIR, \"pdf\")\n",
    "TXT_DIR = path.join(DATA_DIR, \"txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez teniendo los libros en formato PDF, se procede a convertirlos a formato TXT, para poder trabajar con ellos.\n",
    "La conversion ocurre pagina por pagina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing codigo_limpio_robert_cecil_martin.pdf: 100%|██████████| 644/644 [00:13<00:00, 48.38it/s] \n",
      "Processing ingenieria_de_software_sommerville.PDF: 100%|██████████| 792/792 [00:05<00:00, 156.39it/s]\n",
      "Processing ingenieria_de_software_pressman.PDF: 100%|██████████| 810/810 [00:09<00:00, 84.90it/s] \n"
     ]
    }
   ],
   "source": [
    "makedirs(TXT_DIR, exist_ok=True)\n",
    "# obtener todos los archivos pdf dentro de PDF_DIR\n",
    "pdf_files = [f for f in listdir(PDF_DIR) if f.lower().endswith('.pdf')]\n",
    "for pdf in pdf_files:\n",
    "    # leer el archivo pdf\n",
    "    with open(path.join(PDF_DIR, pdf), 'rb') as f:\n",
    "        pdf_reader = PdfReader(f)\n",
    "        # leer el número de páginas en el archivo pdf\n",
    "        pbar = tqdm(range(len(pdf_reader.pages)))\n",
    "        # iterar sobre las páginas en el archivo pdf\n",
    "        page_num = 0\n",
    "        for page in pbar:\n",
    "            pbar.set_description(f'Processing {pdf}')\n",
    "            # obtener el texto de la página eliminando los saltos de línea\n",
    "            text = pdf_reader.pages[page].extract_text().replace('\\n', ' ')\n",
    "            # escribir el texto en un archivo txt\n",
    "            txt_file = f'{pdf[:-4]}_{page_num}.txt'\n",
    "            # generar un archivo txt que contenga el texto de todos los archivos txt de un mismo pdf\n",
    "            main_txt_file = f'{pdf[:-4]}.txt'\n",
    "            with open(path.join(TXT_DIR, txt_file), 'w', encoding='utf-8') as f, open(path.join(TXT_DIR, main_txt_file), 'a', encoding='utf-8') as mwf:\n",
    "                # formatear el texto para normalizarlo\n",
    "                formatted_text = text.replace('\\n', ' ').lower().replace('  ', ' ')\n",
    "                f.write(formatted_text)\n",
    "                mwf.write(formatted_text)\n",
    "                page_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez teniendo todos los archivos .txt se debe obtener el dataset de STS lleno de oraciones relacionadas a la ingenieria de software y un puntaje entre 0 y 1 que valore la similitud entre las oraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se buscara hacer uso de algun modelo para extraer las oraciones del archivo .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.10/site-packages (0.15.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.30.2)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (2.13.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (3.12.2)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.6.3)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.10/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub transformers datasets  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscando los modelos en espanol para extraer automaticamente las oraciones de ingenieria de software encontre este modelo: [hiiamsid/sentence_similarity_spanish_es](https://huggingface.co/hiiamsid/sentence_similarity_spanish_es) el cual potencialmente sera usado para realizarle fine-tuning y asi obtener un modelo final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se busca obtener cada oracion del archivo .txt principal de cada libro e incluirla en un archivo .json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se limpia el texto de cada archivo .txt, se eliminan los saltos de linea, espacios en blanco, caracteres especiales, palabras que no aportan informacion, palabras repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_useless_text(text):\n",
    "    # Regular expressions for detecting useless text patterns\n",
    "    patterns = [\n",
    "        r'>',  # Remove bullet point indicators\n",
    "        r'\\d+\\w*',  # Remove numbers or digits combined with letters\n",
    "        r'\\b\\w\\b',  # Remove single-letter words\n",
    "        r'\\b\\w{1,2}\\b',  # Remove words with one or two characters\n",
    "        r'www\\.[^\\s]+',  # Remove URLs\n",
    "    ]\n",
    "\n",
    "    # Apply each pattern sequentially to remove useless text\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in listdir(TXT_DIR):\n",
    "    if not any(char.isdigit() for char in filename):\n",
    "        # por la convencion de nombres que se tomo, estos archivos son los que contienen todo el texto de su respectivo pdf\n",
    "        with open(TXT_DIR + '/' + filename, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            cleaned_text = remove_useless_text(text)\n",
    "            # Do something with the cleaned text, such as write it to a new file\n",
    "            with open(TXT_DIR + '/cleaned_' + filename, 'w', encoding='utf-8') as cleaned_file:\n",
    "                cleaned_file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con un archivo que contiene texto \"limpio\" se obtienen las oraciones de cada libro y se incluyen en un archivo .json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/abraham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/tmp/ipykernel_372642/2583404250.py:17: DeprecationWarning: invalid escape sequence '\\â'\n",
      "  sentence = codecs.decode(sentence, 'unicode_escape')\n",
      "/tmp/ipykernel_372642/2583404250.py:17: DeprecationWarning: invalid escape sequence '\\!'\n",
      "  sentence = codecs.decode(sentence, 'unicode_escape')\n",
      "/tmp/ipykernel_372642/2583404250.py:17: DeprecationWarning: invalid escape sequence '\\|'\n",
      "  sentence = codecs.decode(sentence, 'unicode_escape')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "\n",
    "# Descargar el tokenizador de oraciones de NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open('/home/abraham/personal/sts-swe/data/txt/cleaned_codigo_limpio_robert_cecil_martin.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "meaningful_sentences = []\n",
    "for sentence in sentences:\n",
    "    # Si la oración tiene más de 5 palabras, la consideramos significativa\n",
    "    if len(sentence.split()) > 5:\n",
    "        # Decodificar la oración para reemplazar los escape sequences con los caracteres correspondientes\n",
    "        sentence = sentence.encode().decode('unicode_escape')\n",
    "        meaningful_sentences.append(sentence)\n",
    "\n",
    "# Crear un objeto JSON y agregar las oraciones significativas como elementos del objeto\n",
    "data = {}\n",
    "data['sentences'] = meaningful_sentences\n",
    "\n",
    "with open('meaningful_sentences.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in ./.venv/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./.venv/lib/python3.10/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.10/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./.venv/lib/python3.10/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./.venv/lib/python3.10/site-packages (from spacy) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./.venv/lib/python3.10/site-packages (from spacy) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting es-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.5.0/es_core_news_sm-3.5.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from es-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 2010322 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m     11\u001b[0m \u001b[39m# Process whole documents\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Tokenize sentence\u001b[39;00m\n\u001b[1;32m     15\u001b[0m sentences \u001b[39m=\u001b[39m [sent\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents]\n",
      "File \u001b[0;32m~/personal/sts-swe/.venv/lib/python3.10/site-packages/spacy/language.py:999\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    979\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    980\u001b[0m     text: Union[\u001b[39mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    983\u001b[0m     component_cfg: Optional[Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[1;32m    985\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[39m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m     doc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_doc(text)\n\u001b[1;32m   1000\u001b[0m     \u001b[39mif\u001b[39;00m component_cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m         component_cfg \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/personal/sts-swe/.venv/lib/python3.10/site-packages/spacy/language.py:1090\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1089\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_doc(doc_like)\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1092\u001b[0m     \u001b[39mreturn\u001b[39;00m Doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\u001b[39m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m~/personal/sts-swe/.venv/lib/python3.10/site-packages/spacy/language.py:1079\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \n\u001b[1;32m   1075\u001b[0m \u001b[39mtext (str): The text to process.\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[39mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length:\n\u001b[0;32m-> 1079\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1080\u001b[0m         Errors\u001b[39m.\u001b[39mE088\u001b[39m.\u001b[39mformat(length\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(text), max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length)\n\u001b[1;32m   1081\u001b[0m     )\n\u001b[1;32m   1082\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(text)\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 2010322 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load Spanish tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"es_core_news_sm\")  \n",
    "# you can change \"es_core_news_sm\" to \"es_core_news_md\" if you have downloaded the medium Spanish model\n",
    "\n",
    "# Read the text file\n",
    "with open('/home/abraham/personal/sts-swe/data/txt/cleaned_codigo_limpio_robert_cecil_martin.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Process whole documents\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize sentence\n",
    "sentences = [sent.text for sent in doc.sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    # Si la oración tiene más de 5 palabras, la consideramos significativa\n",
    "    if len(sentence.split()) > 5:\n",
    "        # Decodificar la oración para reemplazar los escape sequences con los caracteres correspondientes\n",
    "        sentence = sentence.encode().decode('unicode_escape')\n",
    "        meaningful_sentences.append(sentence)\n",
    "\n",
    "# Crear un objeto JSON y agregar las oraciones significativas como elementos del objeto\n",
    "data = {}\n",
    "data['sentences'] = meaningful_sentences\n",
    "\n",
    "with open('meaningful_sentences.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se opta por usar los embeddings del modelo anteriormente mencionado para asi encontrar el contexto de cada oracion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
